{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/conda/envs/pytorch/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from math import pi\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda:0'\n",
    "phase_mod = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFourierFeatureTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of Gaussian Fourier feature mapping.\n",
    "    \"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains\":\n",
    "       https://arxiv.org/abs/2006.10739\n",
    "       https://people.eecs.berkeley.edu/~bmild/fourfeat/index.html\n",
    "    Given an input of size [batches, num_input_channels, width, height],\n",
    "     returns a tensor of size [batches, mapping_dim*2, width, height].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_input_channels=2, mapping_dim=256, scale=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self._num_input_channels = num_input_channels\n",
    "        self.mapping_dim = mapping_dim\n",
    "        self._B = torch.randn((num_input_channels, mapping_dim)) * scale\n",
    "\n",
    "    def forward(self, x, phase=None):\n",
    "        assert x.dim() == 4, 'Expected 4D input (got {}D input)'.format(x.dim())\n",
    "\n",
    "        batches, channels, width, height = x.shape\n",
    "        assert channels == self._num_input_channels,\\\n",
    "            \"Expected input to have {} channels (got {} channels)\".format(self._num_input_channels, channels)\n",
    "\n",
    "        # Make shape compatible for matmul with _B.\n",
    "        # From [B, C, W, H] to [(B*W*H), C].\n",
    "        x = x.permute(0, 2, 3, 1).reshape(batches * width * height, channels)\n",
    "\n",
    "        x = x @ self._B.to(x.device)\n",
    "\n",
    "        # From [(B*W*H), C] to [B, W, H, C]\n",
    "        x = x.view(batches, width, height, self.mapping_dim)\n",
    "        # From [B, W, H, C] to [B, C, W, H]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        if phase is not None:\n",
    "            x = 2 * pi * x + phase\n",
    "        else:\n",
    "            x = 2 * pi * x\n",
    "        \n",
    "        return torch.cat([torch.sin(x), torch.cos(x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFF(nn.Module):\n",
    "    def __init__(self, mapping_size, num_input_channels=2):\n",
    "        super(LFF, self).__init__()\n",
    "        self.ffm = ConLinear(num_input_channels, mapping_size, is_first=True)\n",
    "        self.activation = SinActivation()\n",
    "\n",
    "    def forward(self, x, phase=None):            \n",
    "        x = self.ffm(x)\n",
    "        if phase is not None:\n",
    "            x = self.activation(x + phase)\n",
    "        else:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class ConLinear(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, is_first=False, bias=True):\n",
    "        super(ConLinear, self).__init__()\n",
    "        self.conv = nn.Conv2d(ch_in, ch_out, kernel_size=1, padding=0, bias=bias)\n",
    "        if is_first:\n",
    "            nn.init.uniform_(self.conv.weight, -np.sqrt(9 / ch_in), np.sqrt(9 / ch_in))\n",
    "        else:\n",
    "            nn.init.uniform_(self.conv.weight, -np.sqrt(3 / ch_in), np.sqrt(3 / ch_in))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class SinActivation(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(SinActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x, phase=None):\n",
    "        if phase:\n",
    "            return torch.sin(x + phase)\n",
    "        else:\n",
    "            return torch.sin(x)\n",
    "        \n",
    "        \n",
    "def get_grid(h, w, b=0, norm=True, device='cpu'):\n",
    "    if norm:\n",
    "        xgrid = np.linspace(0, w, num=w) / w\n",
    "        ygrid = np.linspace(0, h, num=h) / h\n",
    "    else:\n",
    "        xgrid = np.linspace(0, w, num=w)\n",
    "        ygrid = np.linspace(0, h, num=h)\n",
    "    xv, yv = np.meshgrid(xgrid, ygrid, indexing='xy')\n",
    "    grid = np.stack([xv, yv], axis=-1)[None]\n",
    "\n",
    "    grid = torch.from_numpy(grid).float().to(device)\n",
    "    if b > 0:\n",
    "        grid = grid.expand(b, -1, -1, -1)  # [Batch, H, W, UV]\n",
    "        return grid.permute(0, 3, 1, 2)  # [Batch, UV, H, W]\n",
    "    else:\n",
    "        return grid[0].permute(2, 0, 1)  # [UV, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 internal_dim,\n",
    "                 num_layers=3,\n",
    "                 act=nn.LeakyReLU(), \n",
    "                 ff_func=LFF,\n",
    "                 phase_mod=False,\n",
    "                 num_input_channels=2,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.lff = ff_func(mapping_dim=internal_dim, num_input_channels=num_input_channels)\n",
    "        self.net = [ConLinear(internal_dim * 2, internal_dim * 2, is_first=False),\n",
    "                    act,\n",
    "        ]\n",
    "        for layer_n in range(num_layers):\n",
    "            self.net.append(ConLinear(internal_dim * 2, internal_dim * 2, is_first=False))\n",
    "            self.net.append(act)\n",
    "    \n",
    "        self.net.append(ConLinear(internal_dim * 2, 3, is_first=False))\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "        \n",
    "        if phase_mod:\n",
    "            self.phase_net = [ff_func(mapping_dim=internal_dim // 2,\n",
    "                                      num_input_channels=3),\n",
    "                              ConLinear(internal_dim,\n",
    "                                        internal_dim,\n",
    "                                        is_first=False),\n",
    "                              act,\n",
    "                              ConLinear(internal_dim,\n",
    "                                        internal_dim,\n",
    "                                        is_first=False),\n",
    "                              act,\n",
    "                              ConLinear(internal_dim,\n",
    "                                        internal_dim,\n",
    "                                        is_first=False),\n",
    "                              act,\n",
    "                              ConLinear(internal_dim,\n",
    "                                        1,\n",
    "                                        is_first=False),]\n",
    "            self.phase_net = nn.Sequential(*self.phase_net)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        if phase_mod:\n",
    "            # coords: list of [spatial_coords:[B,2,H,W], spacetime_coords[B, 3, H, W]]\n",
    "            spatial_coords, time_coords = coords\n",
    "            phase_feats = self.phase_net(time_coords)\n",
    "            fourier_feats = self.lff(spatial_coords, phase_feats)\n",
    "        else:\n",
    "            fourier_feats = self.lff(coords)\n",
    "        return self.net(fourier_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "    def __init__(self, folder, resolution=256, phase_mod=False):\n",
    "        super().__init__()\n",
    "        self.files = sorted(glob(os.path.join(folder, '*.png')))\n",
    "        self.num_frames = len(self.files)\n",
    "        self.resolution = resolution\n",
    "        self.phase_mod = phase_mod\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx])\n",
    "        img = TF.Resize(self.resolution)(img)\n",
    "        img = TF.ToTensor()(img)\n",
    "        \n",
    "        coords = get_grid(*img.shape[-2:], 0, True, 'cpu')\n",
    "        if self.phase_mod:\n",
    "            timestamp = int(self.files[idx].split('/')[-1].split('.')[0].split('_')[-1])\n",
    "            timestamp_float = timestamp / self.num_frames\n",
    "            coords = [coords, torch.cat([coords, coords[[0]] * 0. + timestamp_float], dim=0)]\n",
    "        else:\n",
    "            timestamp = int(self.files[idx].split('/')[-1].split('.')[0].split('_')[-1])\n",
    "            timestamp_float = timestamp / self.num_frames\n",
    "            coords = torch.cat([coords, coords[[0]] * 0. + timestamp_float], dim=0)\n",
    "        return img, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmlp = FMLP(128,\n",
    "            num_layers=8,\n",
    "            act=nn.LeakyReLU(),\n",
    "            ff_func=GaussianFourierFeatureTransform,\n",
    "            phase_mod=phase_mod,\n",
    "            num_input_channels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = FramesDataset('./frames/71', phase_mod=phase_mod, resolution=((256, 256)))\n",
    "dloader = DataLoader(dset, batch_size=4, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(fmlp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53b7a684c0c4bca9a7f49fbf4e36abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 0.027553329967400605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23f83c4c8884501b6d8a6ed173af2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429d3beca4b0461aa05dfb10878e4caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 0.017408519698416485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93916afc146b4129966808af75cd8c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 0.012962188933263806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a0bb7f13c649d6859d7d57db33cf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    losses = []\n",
    "    for img, coords in tqdm(dloader):\n",
    "        img, coords = img.to(device), [coordss.to(device) for coordss in coords]\n",
    "        opt.zero_grad()\n",
    "        out = torch.sigmoid(fmlp(coords))\n",
    "        loss = ((img - out) ** 2).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    print(i, np.mean(losses))\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        out_file = cv2.VideoWriter(f'./output_{i}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 60.0, (256, 256))\n",
    "        coords = get_grid(256, 256, 1, True, device)\n",
    "        for frame in tqdm(np.linspace(-0.2, 1.2, 200)):\n",
    "            coords_stacked = [coords, torch.cat([coords, coords[:, [0]] * 0. + frame], dim=1)]\n",
    "            out = torch.sigmoid(fmlp(coords_stacked))\n",
    "            out_file.write((out[0].permute(1, 2, 0).cpu().data.numpy()[:, :, ::-1] * 255).clip(0, 255).astype(np.uint8))\n",
    "        out_file.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = cv2.VideoWriter('./output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 60.0, (256, 256))\n",
    "coords = get_grid(256, 256, 1, True, device)\n",
    "for frame in tqdm(np.linspace(-0.2, 1.2, 200)):\n",
    "    coords_stacked = [coords, torch.cat([coords, coords[:, [0]] * 0. + frame], dim=1)]\n",
    "    out = torch.sigmoid(fmlp(coords_stacked))\n",
    "    out_file.write((out[0].permute(1, 2, 0).cpu().data.numpy()[:, :, ::-1] * 255).clip(0, 255).astype(np.uint8))\n",
    "out_file.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i output.mp4 output_recoded.mp4 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
